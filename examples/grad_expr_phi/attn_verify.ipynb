{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modeling_modifier import nnscaler_flash_attention_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from torch.autograd import gradcheck\n",
    "\n",
    "# Function and custom attention implementation assumed to be defined or imported\n",
    "\n",
    "# Test parameters\n",
    "batch_size = 2\n",
    "seq_len = 128\n",
    "num_heads = 8\n",
    "head_dim = 64\n",
    "\n",
    "\n",
    "# Random input tensors\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(1024)\n",
    "dtype = torch.float16\n",
    "query_states = torch.randn(\n",
    "    batch_size, num_heads, seq_len, head_dim, \n",
    "    requires_grad=True, dtype=dtype, device=device)\n",
    "key_states = torch.randn(\n",
    "    batch_size, num_heads, seq_len, head_dim, \n",
    "    requires_grad=True, dtype=dtype, device=device)\n",
    "value_states = torch.randn(\n",
    "    batch_size, num_heads, seq_len, head_dim, \n",
    "    requires_grad=True, dtype=dtype, device=device)\n",
    "\n",
    "# Parameters for the attention function\n",
    "dropout = 0.\n",
    "softmax_scale = 1 / math.sqrt(head_dim)\n",
    "causal = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.is_autocast_enabled()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "intermediate_results_dict = {\n",
    "    'custom': {},\n",
    "    'standard_attn': {}\n",
    "}\n",
    "\n",
    "class CustomAttenFunc(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(\n",
    "        ctx: torch.autograd.function.FunctionCtx, \n",
    "        query_states, key_states, value_states, \n",
    "        attn_mask, attn_dropout, training\n",
    "    ):\n",
    "        # query_states: [batch_size, num_heads, query_len, head_dim]\n",
    "        # attn_mask: [batch_size, num_heads, query_len, key_len]\n",
    "        head_dim = query_states.size(-1)\n",
    "\n",
    "        # Compute scaled dot-product attention scores\n",
    "        # orig_attn_weights - [batch_size, num_heads, query_len, query_len]\n",
    "        orig_attn_weights = torch.matmul(query_states, key_states.transpose(-2, -1)) / math.sqrt(head_dim)\n",
    "        \n",
    "        # Apply mask if provided\n",
    "        if attn_mask is not None:\n",
    "            causal_mask = attn_mask[:, :, :, :key_states.shape[-2]]\n",
    "            orig_attn_weights = orig_attn_weights + causal_mask\n",
    "\n",
    "        # Softmax across the last dimension and apply dropout if in training mode\n",
    "        orig_attn_weights = nn.functional.softmax(orig_attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
    "        attn_weights = nn.functional.dropout(orig_attn_weights, p=attn_dropout, training=training)\n",
    "\n",
    "        # Multiply attention weights by value states to get the output\n",
    "        # attn_output - [batch_size, num_heads, query_len, head_dim]\n",
    "        attn_output = torch.matmul(attn_weights, value_states)\n",
    "\n",
    "        # Save for backward pass\n",
    "        ctx.save_for_backward(query_states, key_states, value_states, attn_weights, orig_attn_weights, attn_mask)\n",
    "        ctx.attn_dropout = attn_dropout\n",
    "        ctx.training = training\n",
    "\n",
    "        return attn_output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx: torch.autograd.function.FunctionCtx, grad_output):\n",
    "        query_states, key_states, value_states, attn_weights, orig_attn_weights, attn_mask = ctx.saved_tensors\n",
    "        attn_dropout = ctx.attn_dropout\n",
    "        training = ctx.training\n",
    "\n",
    "        # Compute gradients w.r.t. value states\n",
    "        # attn_weights - [batch_size, num_heads, query_len, query_len]\n",
    "        # grad_output - [batch_size, num_heads, query_len, head_dim]\n",
    "        # grad_value_states - [batch_size, num_heads, query_len, head_dim]\n",
    "        grad_value_states = torch.matmul(attn_weights.transpose(-2, -1), grad_output)\n",
    "\n",
    "        # Compute gradients w.r.t. attention weights\n",
    "        # grad_output - [batch_size, num_heads, query_len, head_dim]\n",
    "        # value_states - [batch_size, num_heads, query_len, head_dim]\n",
    "        # grad_attn_weights - [batch_size, num_heads, query_len, query_len]\n",
    "        grad_attn_weights = torch.matmul(grad_output, value_states.transpose(-2, -1))\n",
    "\n",
    "        # Directly assume grad w.r.t attn_weights is equivalent to grad w.r.t orig_attn_weights because \n",
    "        grad_orig_attn_weights = grad_attn_weights\n",
    "\n",
    "        # Compute gradients w.r.t. matrix multiplication before softmax\n",
    "        # Because the direct operand of Softmax is the matrix multiplication result before dropout, so we use orig_attn_weights here\n",
    "        # grad_orig_attn_mul = grad_orig_attn_weights * orig_attn_weights * (1 - orig_attn_weights)\n",
    "        grad_orig_attn_mul = orig_attn_weights * (\n",
    "            grad_orig_attn_weights - torch.sum(orig_attn_weights * grad_orig_attn_weights, dim=-1, keepdim=True)\n",
    "        )\n",
    "\n",
    "        # ------------------------------------------------\n",
    "        # Save the gradients w.r.t (orig)_attn_weights to files\n",
    "\n",
    "\n",
    "        # -------------------------------------------------\n",
    "\n",
    "\n",
    "        # Compute gradients w.r.t. query and key states\n",
    "        # grad_orig_attn_mul - [batch_size, num_heads, query_len, query_len]\n",
    "        # key_states - [batch_size, num_heads, query_len, head_dim]\n",
    "        # query_states - [batch_size, num_heads, query_len, head_dim]\n",
    "        grad_query_states = torch.matmul(grad_orig_attn_mul, key_states) / math.sqrt(query_states.size(-1))\n",
    "        grad_key_states = torch.matmul(query_states.transpose(-2, -1), grad_orig_attn_mul) / math.sqrt(query_states.size(-1))\n",
    "        grad_key_states = grad_key_states.transpose(-2, -1)\n",
    "\n",
    "        return grad_query_states, grad_key_states, grad_value_states, None, None, None\n",
    "\n",
    "class CaptureAttention(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(\n",
    "        ctx: torch.autograd.function.FunctionCtx, \n",
    "        attn_weights: torch.Tensor,\n",
    "    ):\n",
    "        print('Attention weights:', attn_weights.size())\n",
    "        return attn_weights\n",
    "        \n",
    "    @staticmethod\n",
    "    def backward(ctx: torch.autograd.function.FunctionCtx, grad_output: torch.Tensor):\n",
    "        print('Gradient w.r.t attention weights:', grad_output.size())\n",
    "        with open(\"./attention_gradients.pt\", \"wb\") as f:\n",
    "            torch.save(grad_output, f)\n",
    "        return grad_output\n",
    "\n",
    "\n",
    "def custom_attn(\n",
    "        query_states, key_states, value_states, \n",
    "        attn_mask, attn_dropout, training):\n",
    "    head_dim = query_states.size(-1)\n",
    "    attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(head_dim)\n",
    "\n",
    "    if attn_mask is not None:  # no matter the length, we just slice it\n",
    "        causal_mask = attn_mask[:, :, :, : key_states.shape[-2]]\n",
    "        attn_weights = attn_weights + causal_mask\n",
    "\n",
    "    # upcast attention to fp32\n",
    "    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
    "    attn_weights = CaptureAttention.apply(attn_weights)\n",
    "    attn_weights = nn.functional.dropout(attn_weights, p=attn_dropout, training=training)\n",
    "    attn_output = torch.matmul(attn_weights, value_states)\n",
    "\n",
    "    attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "    return attn_output\n",
    "\n",
    "def standard_attn(\n",
    "        query_states, key_states, value_states, \n",
    "        attn_mask, attn_dropout, training):\n",
    "    head_dim = query_states.size(-1)\n",
    "    attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(head_dim)\n",
    "\n",
    "    if attn_mask is not None:  # no matter the length, we just slice it\n",
    "        causal_mask = attn_mask[:, :, :, : key_states.shape[-2]]\n",
    "        attn_weights = attn_weights + causal_mask\n",
    "\n",
    "    # upcast attention to fp32\n",
    "    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
    "    attn_weights = nn.functional.dropout(attn_weights, p=attn_dropout, training=training)\n",
    "    attn_output = torch.matmul(attn_weights, value_states)\n",
    "\n",
    "    attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "    return attn_output\n",
    "\n",
    "def flash_attn(\n",
    "    query_states, key_states, value_states, \n",
    "    attn_mask, attn_dropout, training\n",
    "):\n",
    "    q_len = query_states.size(2)\n",
    "    query_states = query_states.transpose(1, 2) # query_states - [batch_size, query_len, num_heads, head_dim]\n",
    "    key_states = key_states.transpose(1, 2)\n",
    "    value_states = value_states.transpose(1, 2)\n",
    "\n",
    "    dropout_rate = attn_dropout if training else 0.0\n",
    "    input_dtype = query_states.dtype\n",
    "    query_states = query_states.to(input_dtype)\n",
    "    key_states = key_states.to(input_dtype)\n",
    "    value_states = value_states.to(input_dtype)\n",
    "    \n",
    "    causal = True\n",
    "    attn_output = nnscaler_flash_attention_forward(\n",
    "        query_states, key_states, value_states, attn_mask, q_len, dropout=dropout_rate, causal=causal\n",
    "    )\n",
    "    return attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: torch.Size([2, 8, 128, 128])\n",
      "Gradient w.r.t attention weights: torch.Size([2, 8, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "# Clone inputs to keep gradients independent for each function\n",
    "q_flash = query_states.clone().detach().requires_grad_(True)\n",
    "k_flash = key_states.clone().detach().requires_grad_(True)\n",
    "v_flash = value_states.clone().detach().requires_grad_(True)\n",
    "\n",
    "q_custom = query_states.clone().detach().requires_grad_(True)\n",
    "k_custom = key_states.clone().detach().requires_grad_(True)\n",
    "v_custom = value_states.clone().detach().requires_grad_(True)\n",
    "\n",
    "q_standard = query_states.clone().detach().requires_grad_(True)\n",
    "k_standard = key_states.clone().detach().requires_grad_(True)\n",
    "v_standard = value_states.clone().detach().requires_grad_(True)\n",
    "\n",
    "attention_mask = None\n",
    "flash_attn_mask = None\n",
    "\n",
    "output_flash = flash_attn(q_flash, k_flash, v_flash, attention_mask, dropout, True)\n",
    "\n",
    "# output_custom = CustomAttenFunc.apply(q_custom, k_custom, v_custom, custom_mask, dropout, True)\n",
    "# output_custom = output_custom.transpose(1, 2).contiguous()\n",
    "output_custom = custom_attn(q_custom, k_custom, v_custom, attention_mask, dropout, True)\n",
    "output_standard = standard_attn(q_standard, k_standard, v_standard, attention_mask, dropout, True)\n",
    "\n",
    "# Define a loss function and compute backward\n",
    "loss_custom = output_custom.sum()\n",
    "loss_standard = output_standard.sum()\n",
    "loss_flash = output_flash.sum()\n",
    "\n",
    "loss_custom.backward()\n",
    "loss_standard.backward(w)\n",
    "loss_flash.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Sizes of outputs:\n",
      "Output from flash-attention: (torch.Size([2, 8, 128, 64]), torch.Size([2, 8, 128, 64]), torch.Size([2, 8, 128, 64]))\n",
      "Output from custom function: (torch.Size([2, 8, 128, 64]), torch.Size([2, 8, 128, 64]), torch.Size([2, 8, 128, 64]))\n",
      "Output from standard function: (torch.Size([2, 8, 128, 64]), torch.Size([2, 8, 128, 64]), torch.Size([2, 8, 128, 64]))\n",
      "--------------------------------------------------\n",
      "Output Equivalence check:\n",
      "Flash vs Custom: False\n",
      "Flash vs Standard: False\n",
      "Standard vs Custom: True\n",
      "--------------------------------------------------\n",
      "Loss equivalence check:\n",
      "Flash vs Custom: False\n",
      "Flash vs Standard: False\n",
      "Standard vs Custom: True\n"
     ]
    }
   ],
   "source": [
    "print('-' * 50)\n",
    "print(\"Sizes of outputs:\")\n",
    "print(f\"Output from flash-attention: {q_flash.grad.size(), k_flash.grad.size(), v_flash.grad.size()}\")\n",
    "print(f'Output from custom function: {q_custom.grad.size(), k_custom.grad.size(), v_custom.grad.size()}')\n",
    "print(f'Output from standard function: {q_standard.grad.size(), k_standard.grad.size(), v_standard.grad.size()}')\n",
    "\n",
    "print('-' * 50)\n",
    "print(\"Output Equivalence check:\")\n",
    "print(f\"Flash vs Custom: {torch.allclose(output_flash, output_custom, atol=1e-4)}\")\n",
    "print(f\"Flash vs Standard: {torch.allclose(output_flash, output_standard, atol=1e-4)}\")\n",
    "print(f\"Standard vs Custom: {torch.allclose(output_standard, output_custom, atol=1e-4)}\")\n",
    "\n",
    "print('-' * 50)\n",
    "print(\"Loss equivalence check:\")\n",
    "print(f\"Flash vs Custom: {torch.allclose(loss_flash, loss_custom, atol=1e-4)}\")\n",
    "print(f\"Flash vs Standard: {torch.allclose(loss_flash, loss_standard, atol=1e-4)}\")\n",
    "print(f\"Standard vs Custom: {torch.allclose(loss_standard, loss_custom, atol=1e-4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 1.6431e-01, -1.3013e-01,  7.6807e-01,  ...,  1.5557e+00,\n",
      "           -9.8047e-01, -7.8223e-01],\n",
      "          [-4.1797e-01, -2.8857e-01, -1.2910e+00,  ..., -2.7441e-01,\n",
      "            2.0996e+00,  3.7769e-01],\n",
      "          [-4.5654e-02,  4.2310e-01,  2.7422e+00,  ...,  4.8828e-01,\n",
      "           -1.9102e+00,  4.2188e-01],\n",
      "          ...,\n",
      "          [-3.5889e-01, -1.0791e+00,  1.0559e-01,  ..., -8.3496e-01,\n",
      "           -1.3301e+00, -8.0615e-01],\n",
      "          [-1.6113e+00, -2.3511e-01, -3.9209e-01,  ..., -3.6084e-01,\n",
      "           -2.1716e-01, -2.3633e-01],\n",
      "          [-1.9102e+00,  8.7500e-01, -1.6748e-01,  ...,  2.2375e-01,\n",
      "           -1.9434e+00, -9.2920e-01]],\n",
      "\n",
      "         [[ 2.1448e-01, -5.6055e-01,  2.7002e-01,  ...,  1.5781e+00,\n",
      "           -9.2090e-01, -5.0342e-01],\n",
      "          [ 1.7322e-01, -1.2329e-02, -1.5596e+00,  ..., -6.9434e-01,\n",
      "            7.2168e-01,  8.4277e-01],\n",
      "          [-3.6621e-03,  9.1553e-02,  2.3789e+00,  ..., -1.3062e-02,\n",
      "           -7.7832e-01,  6.0059e-01],\n",
      "          ...,\n",
      "          [-3.9185e-02, -2.7588e-01,  2.6514e-01,  ..., -2.5781e-01,\n",
      "           -8.0225e-01, -5.1270e-02],\n",
      "          [ 5.1086e-02,  9.6680e-02, -5.0244e-01,  ...,  1.7773e+00,\n",
      "           -1.8281e+00,  5.5322e-01],\n",
      "          [-7.1191e-01, -4.7021e-01, -1.6455e-01,  ...,  5.2002e-01,\n",
      "           -1.3330e+00, -4.6338e-01]],\n",
      "\n",
      "         [[ 4.1309e-01, -2.8564e-01, -1.3281e-01,  ...,  1.0283e+00,\n",
      "           -6.4844e-01, -6.4551e-01],\n",
      "          [ 1.3025e-01, -6.9214e-02, -1.5947e+00,  ..., -3.7524e-01,\n",
      "            2.3059e-01,  8.6475e-01],\n",
      "          [ 8.0908e-01, -2.2656e-01,  1.2793e+00,  ..., -5.4346e-01,\n",
      "            1.1201e+00,  7.0264e-01],\n",
      "          ...,\n",
      "          [-1.0687e-01,  1.9263e-01,  6.3965e-01,  ...,  1.6162e-01,\n",
      "           -5.2930e-01,  7.7686e-01],\n",
      "          [-1.1855e+00,  9.7778e-02, -6.2793e-01,  ..., -2.3767e-01,\n",
      "           -3.3057e-01, -2.2546e-01],\n",
      "          [-1.4922e+00, -6.7041e-01, -1.0889e-01,  ..., -1.7949e+00,\n",
      "           -1.0144e-01,  7.8735e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.1973e-02, -1.2398e-02, -8.2397e-04,  ..., -7.9346e-03,\n",
      "           -1.7822e-02,  4.4556e-03],\n",
      "          [-4.9438e-03, -7.4005e-04, -8.5449e-04,  ...,  8.0566e-03,\n",
      "           -8.5449e-04, -9.8877e-03],\n",
      "          [ 1.2329e-02, -5.8594e-03, -3.4180e-03,  ..., -4.8828e-04,\n",
      "           -1.0376e-03,  1.4648e-03],\n",
      "          ...,\n",
      "          [-1.2939e-02, -1.5472e-02, -9.0179e-03,  ...,  4.8828e-04,\n",
      "           -2.5482e-02,  3.9062e-03],\n",
      "          [-7.8735e-03,  2.7161e-03, -3.2959e-03,  ...,  4.0894e-03,\n",
      "           -2.8687e-03,  9.8343e-03],\n",
      "          [ 1.0376e-03,  1.0376e-02,  7.6294e-03,  ...,  1.4648e-03,\n",
      "            1.8311e-03,  6.1035e-03]],\n",
      "\n",
      "         [[ 5.6152e-03, -9.7656e-03, -6.3477e-03,  ..., -9.7046e-03,\n",
      "           -1.5259e-03, -4.8828e-03],\n",
      "          [-2.2888e-03, -4.3945e-03, -7.0190e-04,  ...,  2.4872e-03,\n",
      "            2.3804e-03, -1.0925e-02],\n",
      "          [ 7.3853e-02, -2.1851e-02,  2.2095e-02,  ..., -5.5664e-02,\n",
      "            2.7588e-02, -1.0681e-02],\n",
      "          ...,\n",
      "          [-1.8311e-03, -5.9814e-03, -5.5237e-03,  ...,  6.4697e-03,\n",
      "           -7.2021e-03, -3.9291e-04],\n",
      "          [-1.4496e-03,  6.7139e-04, -1.2207e-03,  ...,  5.7983e-04,\n",
      "           -7.6294e-04,  3.2959e-03],\n",
      "          [-4.8828e-04,  7.5684e-03,  6.3477e-03,  ...,  1.4648e-03,\n",
      "            3.8147e-04,  2.8076e-03]],\n",
      "\n",
      "         [[ 1.5259e-05,  1.2207e-04,  1.2207e-04,  ...,  6.1035e-05,\n",
      "            2.2888e-05,  0.0000e+00],\n",
      "          [ 7.6294e-05,  0.0000e+00,  6.1035e-05,  ..., -6.1035e-05,\n",
      "            0.0000e+00, -1.2207e-04],\n",
      "          [ 6.1035e-05,  1.2207e-04,  1.2207e-04,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [-3.0518e-05,  0.0000e+00, -6.1035e-05,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  7.2479e-05],\n",
      "          [-6.1035e-05,  9.1553e-05,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           -5.3406e-05, -1.2207e-04],\n",
      "          [ 1.5259e-05,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            3.0518e-05, -1.2207e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 1.7705e+00,  1.7461e+00,  1.6445e+00,  ..., -1.1182e+00,\n",
      "            8.3545e-01, -2.1094e-01],\n",
      "          [ 8.4326e-01,  2.6074e+00,  8.9941e-01,  ..., -8.3691e-01,\n",
      "            9.7021e-01, -4.7729e-01],\n",
      "          [ 6.1963e-01, -2.1265e-01,  2.9663e-01,  ..., -1.0144e-01,\n",
      "           -1.1914e+00,  9.9121e-02],\n",
      "          ...,\n",
      "          [ 2.1191e+00,  1.3647e-01,  1.4355e+00,  ..., -2.4084e-01,\n",
      "            1.1172e+00,  8.7402e-01],\n",
      "          [ 8.7305e-01, -1.4038e-01,  7.9004e-01,  ...,  3.5254e-01,\n",
      "           -1.6885e+00,  5.6250e-01],\n",
      "          [-8.2080e-01, -3.5889e-01, -3.8513e-02,  ...,  7.2070e-01,\n",
      "            7.8223e-01, -7.0996e-01]],\n",
      "\n",
      "         [[ 1.6621e+00,  4.3823e-01,  5.7178e-01,  ..., -3.2178e-01,\n",
      "            6.8164e-01, -5.3418e-01],\n",
      "          [ 6.3135e-01,  2.1504e+00,  7.5781e-01,  ..., -1.1016e+00,\n",
      "            8.3447e-01, -4.3945e-01],\n",
      "          [ 2.5244e-01, -5.2368e-02,  5.2344e-01,  ...,  5.2686e-01,\n",
      "           -1.0928e+00, -1.8445e-01],\n",
      "          ...,\n",
      "          [ 1.9385e+00,  1.4795e-01,  1.3184e+00,  ..., -4.0863e-02,\n",
      "            9.2529e-01,  7.0312e-01],\n",
      "          [ 5.4395e-01,  5.9570e-02,  1.4893e+00,  ...,  2.7588e-01,\n",
      "           -4.8096e-01, -2.0435e-01],\n",
      "          [-2.9248e-01, -2.2241e-01,  4.6191e-01,  ...,  7.3389e-01,\n",
      "            4.6094e-01,  2.8748e-02]],\n",
      "\n",
      "         [[ 6.7480e-01, -3.4473e-01,  3.6182e-01,  ...,  5.3418e-01,\n",
      "            4.8071e-01, -7.0801e-01],\n",
      "          [ 6.0889e-01,  6.6992e-01, -1.2021e+00,  ..., -1.5527e-01,\n",
      "            5.8691e-01,  3.8110e-01],\n",
      "          [-8.1201e-01,  6.3867e-01, -4.1919e-01,  ...,  8.0859e-01,\n",
      "           -4.9097e-01, -8.5254e-01],\n",
      "          ...,\n",
      "          [ 1.3662e+00,  2.4365e-01,  7.4854e-01,  ..., -6.3965e-02,\n",
      "            7.7783e-01,  6.3525e-01],\n",
      "          [ 6.3428e-01, -1.1536e-02,  8.7988e-01,  ...,  4.3652e-01,\n",
      "           -8.7695e-01,  4.1870e-01],\n",
      "          [ 1.4375e+00, -2.6581e-02,  9.1064e-01,  ..., -3.0075e-02,\n",
      "           -8.5840e-01,  1.9128e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 7.3853e-03, -1.7090e-03, -6.0272e-03,  ...,  4.3945e-03,\n",
      "           -2.4414e-04,  1.2756e-02],\n",
      "          [ 1.4648e-03, -1.0986e-03, -1.7090e-03,  ..., -1.7996e-03,\n",
      "           -1.4038e-03, -1.2817e-03],\n",
      "          [ 4.3640e-03, -9.7656e-04, -1.4648e-03,  ...,  1.8005e-03,\n",
      "           -3.2959e-03, -8.8501e-04],\n",
      "          ...,\n",
      "          [ 4.7760e-03,  8.5449e-04, -6.1646e-03,  ..., -1.7395e-03,\n",
      "            4.2725e-04, -1.1368e-02],\n",
      "          [ 1.3306e-02, -8.5449e-04, -7.3242e-03,  ..., -3.2959e-03,\n",
      "            6.4087e-03, -2.4414e-02],\n",
      "          [ 4.0283e-03,  1.5869e-03,  5.9204e-03,  ...,  1.7700e-03,\n",
      "            5.5542e-03, -6.2256e-03]],\n",
      "\n",
      "         [[ 6.7932e-02,  1.5961e-02, -8.5144e-02,  ...,  6.1127e-02,\n",
      "           -2.2385e-02,  1.4783e-01],\n",
      "          [ 2.2888e-03, -1.5869e-03, -1.3916e-02,  ...,  9.0332e-03,\n",
      "           -1.2543e-02, -7.9346e-03],\n",
      "          [ 1.6388e-02,  3.6011e-03, -2.7466e-03,  ...,  1.1597e-03,\n",
      "           -2.9495e-02, -8.6670e-03],\n",
      "          ...,\n",
      "          [-2.1362e-04, -5.0354e-03,  1.6212e-03,  ...,  2.4414e-03,\n",
      "           -1.7090e-03,  4.0283e-03],\n",
      "          [ 1.7090e-03, -4.8828e-04,  6.1035e-05,  ..., -1.1597e-03,\n",
      "            1.6174e-03, -3.4180e-03],\n",
      "          [-1.7090e-03, -1.2207e-04,  3.2959e-03,  ..., -2.4719e-03,\n",
      "            2.9297e-03, -3.6621e-03]],\n",
      "\n",
      "         [[-9.1553e-05,  0.0000e+00, -1.2207e-04,  ...,  1.2207e-04,\n",
      "           -6.1035e-05, -1.2207e-04],\n",
      "          [ 0.0000e+00,  0.0000e+00, -6.1035e-05,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  4.5776e-05],\n",
      "          [ 1.5259e-04,  2.4414e-04,  1.2207e-04,  ..., -6.1035e-05,\n",
      "            5.0545e-05, -1.2207e-04],\n",
      "          ...,\n",
      "          [-6.1035e-05,  1.2207e-04,  0.0000e+00,  ...,  6.1035e-05,\n",
      "           -6.1035e-05,  0.0000e+00],\n",
      "          [-6.1035e-05,  0.0000e+00,  0.0000e+00,  ..., -6.1035e-05,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 1.2207e-04,  6.1035e-05, -1.2207e-04,  ...,  2.4414e-04,\n",
      "            6.1035e-05, -6.1035e-05]]]], device='cuda:0', dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(output_flash - output_custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equality check for gradients:\n",
      "--------------------------------------------------\n",
      "Query:\n",
      "Custom vs Standard: True\n",
      "--------------------------------------------------\n",
      "Key:\n",
      "Custom vs Standard: True\n",
      "--------------------------------------------------\n",
      "Value:\n",
      "Custom vs Standard: True\n"
     ]
    }
   ],
   "source": [
    "print(f\"Equality check for gradients:\")\n",
    "\n",
    "print('-' * 50)\n",
    "print(f\"Query:\")\n",
    "print(f'Custom vs Standard: {torch.allclose(q_custom.grad, q_standard.grad, atol=1e-4)}')\n",
    "\n",
    "print('-' * 50)\n",
    "print(f\"Key:\")\n",
    "print(f'Custom vs Standard: {torch.allclose(k_custom.grad, k_standard.grad, atol=1e-4)}')\n",
    "\n",
    "print('-' * 50)\n",
    "print(f\"Value:\")\n",
    "print(f'Custom vs Standard: {torch.allclose(v_custom.grad, v_standard.grad, atol=1e-4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',\n",
      "       dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(k_custom.grad - k_standard.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0449, -0.4001,  0.1593,  0.0342,  0.4521,  0.4736, -0.2952, -1.1221,\n",
       "        -0.2947,  0.1538, -0.2749,  0.2402,  0.0555,  0.2164, -0.3953, -0.5356,\n",
       "         0.2583,  0.7163,  0.2443, -0.1348,  0.2311,  0.0945, -0.2944,  0.4666,\n",
       "        -0.1801, -0.2402,  0.6650,  0.6274,  0.8462,  0.2656,  1.0762, -0.0509,\n",
       "         0.3887, -0.0023,  0.8545,  0.0223, -0.1755, -0.5200,  1.1436,  0.7930,\n",
       "        -1.0967, -0.4268,  0.2148, -0.0367, -0.0276, -0.1140, -0.1279, -0.2732,\n",
       "        -0.3071,  0.3386,  0.5171,  0.4324, -0.4165,  0.5942,  0.9927, -0.4075,\n",
       "         0.0721, -0.3181,  0.3213, -0.1255, -0.0927, -0.2695,  0.5225,  0.3623],\n",
       "       device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnscaler",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
